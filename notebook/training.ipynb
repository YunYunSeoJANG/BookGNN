{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment Setting\n",
    "I haven't checked this in colab environment. Please inform me whether this code works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Load data (First run)\n",
    "If you already save `graph_kcore.gpickle` in previous run, please move to **1.2. Fast Start**. (It might save your ~15 mins.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Read interactions from preprocessed json file in data_preprocessing.ipynb\n",
    "with open('../datasets/interactions_poetry.json') as f:\n",
    "\tusers = pd.DataFrame(json.loads(line) for line in f)\n",
    "\n",
    "# is it necessary? it doesn't seem to be used\n",
    "with open('../datasets/books_poetry.json') as f:\n",
    "\titems = pd.DataFrame(json.loads(line) for line in f)\n",
    "\n",
    "# Make an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add user nodes to the Graph (377799 users)\n",
    "G.add_nodes_from(users['user_id'], type = 'user')\n",
    "print('Num nodes:', G.number_of_nodes(), '. Num edges:', G.number_of_edges()) # Num nodes: 377799 . Num edges: 0\n",
    "\n",
    "# Add item nodes to the graph (36514 books)\n",
    "G.add_nodes_from(users['book_id'], type = 'book')\n",
    "\n",
    "# Make a bipartite graph\n",
    "edges = [(row['user_id'], row['book_id']) for index, row in users.iterrows()]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "print('Num nodes:', G.number_of_nodes(), '. Num edges:', G.number_of_edges()) # Num nodes: 414313 . Num edges: 2734350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find kcore of given graph; it might take some time(7 mins in my case)\n",
    "# kcore : a maximal subgraph that contains nodes of degree k or more\n",
    "kcore = 30\n",
    "G = nx.k_core(G, kcore)\n",
    "print('Num nodes:', G.number_of_nodes(), '. Num edges:', G.number_of_edges()) # Num nodes: 17738 . Num edges: 767616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print kcore graph only top 5 nodes\n",
    "list(G.nodes(data=True))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# save the graph (To omit the time-consuming process)\n",
    "os.makedirs('../assets', exist_ok=True)\n",
    "with open('../assets/graph_kcore.gpickle', 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Fast start\n",
    "You can start from here if you already save `graph_kcore.gpickle` in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# load the graph\n",
    "with open('../assets/graph_kcore.gpickle', 'rb') as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_nodes, n_edges = G.number_of_nodes(), G.number_of_edges()\n",
    "\n",
    "sorted_nodes = sorted(list(G.nodes()))\n",
    "\n",
    "# change book_ids and user_ids into integers\n",
    "node2id = dict(zip(sorted_nodes, np.arange(n_nodes)))\n",
    "id2node = dict(zip(np.arange(n_nodes), sorted_nodes))\n",
    "\n",
    "G = nx.relabel_nodes(G, node2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.nodes(data=True))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user and item indices from the graph\n",
    "user_idx = [i for i, v in enumerate(node2id.keys()) if 'user' in G.nodes[i]['type']]\n",
    "item_idx = [i for i, v in enumerate(node2id.keys()) if 'book' in G.nodes[i]['type']]\n",
    "n_user = len(user_idx)\n",
    "n_item = len(item_idx)\n",
    "\n",
    "print(n_user, n_item) # 11842 5896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "num_nodes = G.number_of_nodes() # 17738 = 11842 + 5896\n",
    "num_edges = G.number_of_edges() # 767616\n",
    "print(num_nodes, num_edges) # 17738 767616\n",
    "edge_idx = torch.Tensor(np.array(G.edges()).T) # torch.Size([2, 767616]) = [2, n_edges]\n",
    "graph_data = Data(edge_index = edge_idx, num_nodes = num_nodes)\n",
    "\n",
    "# convert to train/val/test splits\n",
    "transform = RandomLinkSplit(\n",
    "    is_undirected=True, \n",
    "    add_negative_train_samples=False, \n",
    "    neg_sampling_ratio=0,\n",
    "    num_val=0.15, num_test=0.15\n",
    ")\n",
    "train_split, val_split, test_split = transform(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It was hard to understand for me... refer to https://github.com/pyg-team/pytorch_geometric/discussions/5189#discussioncomment-3378727 :)\n",
    "\n",
    "# Edge index: message passing edges\n",
    "# : testing edges (not the real edges)\n",
    "train_split.edge_index = train_split.edge_index.type(torch.int64)\n",
    "val_split.edge_index = val_split.edge_index.type(torch.int64)\n",
    "test_split.edge_index = test_split.edge_index.type(torch.int64)\n",
    "\n",
    "# Edge label index: supervision edges\n",
    "# : ground truth edges\n",
    "train_split.edge_label_index = train_split.edge_label_index.type(torch.int64)\n",
    "val_split.edge_label_index = val_split.edge_label_index.type(torch.int64)\n",
    "test_split.edge_label_index = test_split.edge_label_index.type(torch.int64)\n",
    "\n",
    "print(f\"Train set has {train_split.edge_label_index.shape[1]} positives supervision edges\")\n",
    "print(f\"Validation set has {val_split.edge_label_index.shape[1]} positive supervision edges\")\n",
    "print(f\"Test set has {test_split.edge_label_index.shape[1]} positive supervision edges\")\n",
    "\n",
    "print(f\"Train set has {train_split.edge_index.shape[1]} message passing edges\")\n",
    "print(f\"Validation set has {val_split.edge_index.shape[1]} message passing edges\")\n",
    "print(f\"Test set has {test_split.edge_index.shape[1]} message passing edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.nn import Embedding, ModuleList, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv, GATConv, SAGEConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
    "\n",
    "\n",
    "# loss function for Bayesian Personalized Ranking\n",
    "class BPRLoss(_Loss):\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).sum()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2) \n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs\n",
    "\n",
    "# Graph Convolutional Network, U can use various convolutional layers, such as LGC, GAT, SAGE\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int,\n",
    "        alpha: Optional[Union[float, Tensor]] = None,\n",
    "        alpha_learnable = False,\n",
    "        conv_layer = \"LGC\",\n",
    "        name = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        '''\n",
    "        num_nodes: number of nodes in the graph\n",
    "        embedding_dim: dimension of the node embedding\n",
    "        num_layers: number of convolutional layers\n",
    "        alpha: alpha value for the convex combination of the embeddings\n",
    "        alpha_learnable: whether to learn the alpha values\n",
    "        conv_layer: type of convolutional layer to use (LGC, GAT, SAGE)\n",
    "        name: name of the model\n",
    "        **kwargs: additional arguments for the convolutional layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "        alpha_string = \"alpha\" if alpha_learnable else \"\"\n",
    "        self.name = f\"LGCN_{conv_layer}_{num_layers}_e{embedding_dim}_nodes{num_nodes}_{alpha_string}\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if alpha_learnable == True: # False\n",
    "          alpha_vals = torch.rand(num_layers+1)\n",
    "          alpha = nn.Parameter(alpha_vals/torch.sum(alpha_vals))\n",
    "          print(f\"Alpha learnable, initialized to: {alpha.softmax(dim=-1)}\")\n",
    "        else:\n",
    "          if alpha is None:\n",
    "              alpha = 1. / (num_layers + 1)\n",
    "\n",
    "          if isinstance(alpha, Tensor):\n",
    "              assert alpha.size(0) == num_layers + 1\n",
    "          else:\n",
    "              alpha = torch.tensor([alpha] * (num_layers + 1)) # alpha = torch.Tensor([1/4, 1/4, 1/4, 1/4]) if num_layers = 3\n",
    "\n",
    "        self.register_buffer('alpha', alpha)\n",
    "\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "\n",
    "        # initialize convolutional layers \n",
    "        self.conv_layer = conv_layer\n",
    "        if conv_layer == \"LGC\":  \n",
    "          self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
    "        elif conv_layer == \"GAT\":\n",
    "          # initialize Graph Attention layer with multiple heads\n",
    "          # initialize linear layers to aggregate heads \n",
    "          n_heads = 5\n",
    "          self.convs = ModuleList(\n",
    "              [GATConv(in_channels = embedding_dim, out_channels = embedding_dim, heads = n_heads, dropout = 0.5, **kwargs) for _ in range(num_layers)]\n",
    "          )\n",
    "          self.linears = ModuleList([Linear(n_heads * embedding_dim, embedding_dim) for _ in range(num_layers)])\n",
    "\n",
    "        elif conv_layer == \"SAGE\":\n",
    "          #  initialize GraphSAGE conv\n",
    "          self.convs = ModuleList(\n",
    "              [SAGEConv(in_channels = embedding_dim, out_channels = embedding_dim, **kwargs) for _ in range(num_layers)]\n",
    "          )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
    "        x = self.embedding.weight\n",
    "        \n",
    "        weights = self.alpha.softmax(dim=-1)\n",
    "        out = x * weights[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            if self.conv_layer == \"GAT\":\n",
    "              x = self.linears[i](x)\n",
    "            out = out + x * weights[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def initialize_embeddings(self, data):\n",
    "      # initialize with the data node features\n",
    "        self.embedding.weight.data.copy_(data.node_feature)\n",
    "\n",
    "\n",
    "    def forward(self, edge_index: Adj,\n",
    "                edge_label_index: OptTensor = None) -> Tensor:\n",
    "        if edge_label_index is None:\n",
    "            if isinstance(edge_index, SparseTensor):\n",
    "                edge_label_index = torch.stack(edge_index.coo()[:2], dim=0)\n",
    "            else:\n",
    "                edge_label_index = edge_index\n",
    "\n",
    "        out = self.get_embedding(edge_index)\n",
    "\n",
    "        return self.predict_link_embedding(out, edge_label_index)\n",
    "\n",
    "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
    "                     prob: bool = False) -> Tensor:\n",
    "       \n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred if prob else pred.round()\n",
    "    \n",
    "    def predict_link_embedding(self, embed: Adj, edge_label_index: Adj) -> Tensor: \n",
    "\n",
    "        embed_src = embed[edge_label_index[0]]\n",
    "        embed_dst = embed[edge_label_index[1]]\n",
    "        return (embed_src * embed_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
    "                  dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
    "        out_src = out_dst = self.get_embedding(edge_index)\n",
    "\n",
    "        if src_index is not None:\n",
    "            out_src = out_src[src_index]\n",
    "\n",
    "        if dst_index is not None:\n",
    "            out_dst = out_dst[dst_index]\n",
    "\n",
    "        pred = out_src @ out_dst.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "\n",
    "        if dst_index is not None:  # Map local top-indices to original indices.\n",
    "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
    "\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
    "                       **kwargs) -> Tensor:\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
    "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)\n",
    "    \n",
    "    def bpr_loss(self, pos_scores, neg_scores):\n",
    "      return - torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
    "                f'{self.embedding_dim}, num_layers={self.num_layers})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative_edges_nocheck(data, num_users, num_items, device = None):\n",
    "  # note computationally inefficient to check that these are indeed negative edges\n",
    "  users = data.edge_label_index[0, :]\n",
    "  items = torch.randint(num_users, num_users + num_items - 1, size = data.edge_label_index[1, :].size())\n",
    "\n",
    "  if users.get_device() != -1: # on gpu \n",
    "    items = items.to(device)\n",
    "\n",
    "  neg_edge_index = torch.stack((users, items), dim = 0)\n",
    "  neg_edge_label = torch.zeros(neg_edge_index.shape[1])\n",
    "\n",
    "  if neg_edge_index.get_device() != -1: # on gpu \n",
    "    neg_edge_label = neg_edge_label.to(device)\n",
    "    \n",
    "  return neg_edge_index, neg_edge_label\n",
    "\n",
    "def sample_negative_edges(data, num_users, num_items, device=None):\n",
    "    positive_users, positive_items = data.edge_label_index\n",
    "    #print(num_users, num_items)\n",
    "    #print(positive_users, positive_items)\n",
    "\n",
    "    positive_users=torch.clamp(positive_users, max=num_users-1, min = 0)\n",
    "    positive_items=torch.clamp(positive_items, max=num_items-1, min = 0)\n",
    "\n",
    "    # Create a mask tensor with the shape (num_users, num_items)\n",
    "    mask = torch.zeros(num_users, num_items, device=device, dtype=torch.bool)\n",
    "    mask[positive_users, positive_items] = True\n",
    "\n",
    "    # Flatten the mask tensor and get the indices of the negative edges\n",
    "    flat_mask = mask.flatten()\n",
    "    negative_indices = torch.where(~flat_mask)[0]\n",
    "\n",
    "    # Sample negative edges from the negative_indices tensor\n",
    "    sampled_negative_indices = negative_indices[\n",
    "        torch.randint(0, negative_indices.size(0), size=(positive_users.size(0),), device=device)\n",
    "    ]\n",
    "\n",
    "    # Convert the indices back to users and items tensors\n",
    "    users = torch.floor_divide(sampled_negative_indices, num_items)\n",
    "    items = torch.remainder(sampled_negative_indices, num_items)\n",
    "    items = items + num_users\n",
    "\n",
    "    neg_edge_index = torch.stack((users, items), dim=0)\n",
    "    neg_edge_label = torch.zeros(neg_edge_index.shape[1], device=device)\n",
    "\n",
    "    return neg_edge_index, neg_edge_label\n",
    "\n",
    "def sample_hard_negative_edges(data, model, num_users, num_items, device=None, batch_size=500, frac_sample = 1):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embedding(data.edge_index)\n",
    "        users_embeddings = embeddings[:num_users].to(device)\n",
    "        items_embeddings = embeddings[num_users:].to(device)\n",
    "\n",
    "    positive_users, positive_items = data.edge_label_index\n",
    "    num_edges = positive_users.size(0)\n",
    "\n",
    "    positive_users=torch.clamp(positive_users, max=num_users-1, min = 0)\n",
    "    positive_items=torch.clamp(positive_items, max=num_items-1, min = 0)\n",
    "\n",
    "    # Create a boolean mask for all the positive edges\n",
    "    positive_mask = torch.zeros(num_users, num_items, device=device, dtype=torch.bool)\n",
    "    positive_mask[positive_users, positive_items] = True\n",
    "\n",
    "    neg_edges_list = []\n",
    "    neg_edge_label_list = []\n",
    "\n",
    "    for batch_start in range(0, num_edges, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_edges)\n",
    "\n",
    "        batch_scores = torch.matmul(\n",
    "            users_embeddings[positive_users[batch_start:batch_end]], items_embeddings.t()\n",
    "        )\n",
    "\n",
    "        # Set the scores of the positive edges to negative infinity\n",
    "        batch_scores[positive_mask[positive_users[batch_start:batch_end]]] = -float(\"inf\")\n",
    "\n",
    "        # Select the top k highest scoring negative edges for each playlist in the current batch\n",
    "        # do 0.99 to filter out all pos edges which will be at the end \n",
    "        _, top_indices = torch.topk(batch_scores, int(frac_sample * 0.99 * num_items), dim=1)\n",
    "        selected_indices = torch.randint(0, int(frac_sample * 0.99 *num_items), size = (batch_end - batch_start, ))\n",
    "        top_indices_selected = top_indices[torch.arange(batch_end - batch_start), selected_indices] + n_user\n",
    "\n",
    "        # Create the negative edges tensor for the current batch\n",
    "        neg_edges_batch = torch.stack(\n",
    "            (positive_users[batch_start:batch_end], top_indices_selected), dim=0\n",
    "        )\n",
    "        neg_edge_label_batch = torch.zeros(neg_edges_batch.shape[1], device=device)\n",
    "\n",
    "        neg_edges_list.append(neg_edges_batch)\n",
    "        neg_edge_label_list.append(neg_edge_label_batch)\n",
    "\n",
    "    # Concatenate the batch tensors\n",
    "    neg_edges = torch.cat(neg_edges_list, dim=1)\n",
    "    neg_edge_label = torch.cat(neg_edge_label_list)\n",
    "\n",
    "    return neg_edges, neg_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(data, model, k = 300, batch_size = 64, device = None):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embedding(data.edge_index)\n",
    "        user_embeddings = embeddings[:n_user]\n",
    "        item_embeddings = embeddings[n_user:]\n",
    "\n",
    "    hits_list = []\n",
    "    relevant_counts_list = []\n",
    "\n",
    "    data.edge_index[1,:]=torch.clamp(data.edge_index[1,:], max=n_item-1, min = 0)\n",
    "\n",
    "    for batch_start in range(0, n_user, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, n_user)\n",
    "        batch_user_embeddings = user_embeddings[batch_start:batch_end]\n",
    "\n",
    "        # Calculate scores for all possible item pairs\n",
    "        scores = torch.matmul(batch_user_embeddings, item_embeddings.t())\n",
    "\n",
    "        # Set the scores of message passing edges to negative infinity\n",
    "        mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
    "        scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices]] = -float(\"inf\")\n",
    "\n",
    "        # Find the top k highest scoring items for each playlist in the batch\n",
    "        _, top_k_indices = torch.topk(scores, k, dim=1)\n",
    "\n",
    "        # Ground truth supervision edges\n",
    "        ground_truth_edges = data.edge_label_index\n",
    "        ground_truth_edges[1,:]=torch.clamp(ground_truth_edges[1,:], max=n_item-1, min = 0)\n",
    "\n",
    "        # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
    "        mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
    "        gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
    "        mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices]] = True\n",
    "\n",
    "        # Check how many of the top k items are in the ground truth supervision edges\n",
    "        hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
    "        hits_list.append(hits)\n",
    "\n",
    "        # Calculate the total number of relevant items for each playlist in the batch\n",
    "        relevant_counts = torch.bincount(ground_truth_edges[0, gt_indices] - batch_start, minlength=batch_end - batch_start)\n",
    "        relevant_counts_list.append(relevant_counts)\n",
    "\n",
    "    # Compute recall@k\n",
    "    hits_tensor = torch.cat(hits_list, dim=0)\n",
    "    relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
    "    # Handle division by zero case\n",
    "    recall_at_k = torch.where(\n",
    "        relevant_counts_tensor != 0,\n",
    "        hits_tensor.true_divide(relevant_counts_tensor),\n",
    "        torch.ones_like(hits_tensor)\n",
    "    )\n",
    "    # take average\n",
    "    recall_at_k = torch.mean(recall_at_k)\n",
    "\n",
    "    if recall_at_k.numel() == 1:\n",
    "        return recall_at_k.item()\n",
    "    else:\n",
    "        raise ValueError(\"recall_at_k contains more than one item.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def metrics(labels, preds):\n",
    "  roc = roc_auc_score(labels.flatten().cpu().numpy(), preds.flatten().data.cpu().numpy())\n",
    "  return roc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "\n",
    "# Train\n",
    "def train(datasets, model, optimizer, loss_fn, args, neg_samp = \"random\"):\n",
    "  print(f\"Beginning training for {model.name}\")\n",
    "\n",
    "  train_data = datasets[\"train\"]\n",
    "  val_data = datasets[\"val\"]\n",
    "\n",
    "  stats = {\n",
    "      'train': {\n",
    "        'loss': [],\n",
    "        'roc' : []\n",
    "      },\n",
    "      'val': {\n",
    "        'loss': [],\n",
    "        'recall': [], \n",
    "        'roc' : []\n",
    "      }\n",
    "\n",
    "  }\n",
    "  val_neg_edge, val_neg_label = None, None\n",
    "  for epoch in range(args[\"epochs\"]): # loop over each epoch \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # obtain negative sample\n",
    "    if neg_samp == \"random\":\n",
    "      neg_edge_index, neg_edge_label = sample_negative_edges(train_data, n_user, n_item, args[\"device\"])\n",
    "    elif neg_samp == \"hard\":\n",
    "      if epoch % 5 == 0: \n",
    "        neg_edge_index, neg_edge_label = sample_hard_negative_edges(\n",
    "            train_data, model, n_user, n_item, args[\"device\"], batch_size = 500, \n",
    "            frac_sample = 1 - (0.5 * epoch / args[\"epochs\"])\n",
    "        )\n",
    "    # calculate embedding\n",
    "    embed = model.get_embedding(train_data.edge_index)\n",
    "    # calculate pos, negative scores using embedding\n",
    "    pos_scores = model.predict_link_embedding(embed, train_data.edge_label_index)\n",
    "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
    "\n",
    "    # concatenate pos, neg scores together and evaluate loss \n",
    "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
    "    labels = torch.cat((train_data.edge_label, neg_edge_label), dim = 0)\n",
    "\n",
    "    # calculate loss function \n",
    "    if loss_fn == \"BCE\": \n",
    "      loss = model.link_pred_loss(scores, labels)\n",
    "    elif loss_fn == \"BPR\":\n",
    "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
    "    \n",
    "    train_roc = metrics(labels, scores)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_roc, val_neg_edge, val_neg_label = test(\n",
    "        model, val_data, loss_fn, neg_samp, epoch, val_neg_edge, val_neg_label\n",
    "    )\n",
    "\n",
    "    stats['train']['loss'].append(loss)\n",
    "    stats['train']['roc'].append(train_roc)\n",
    "    stats['val']['loss'].append(val_loss)\n",
    "    stats['val']['roc'].append(val_roc)\n",
    "\n",
    "    print(f\"Epoch {epoch}; Train loss {loss}; Val loss {val_loss}; Train ROC {train_roc}; Val ROC {val_roc}\")\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "      # calculate recall @ K\n",
    "      val_recall = recall_at_k(val_data, model, k = 300, device = args[\"device\"])\n",
    "      print(f\"Val recall {val_recall}\")\n",
    "      stats['val']['recall'].append(val_recall)\n",
    "\n",
    "    if epoch % 20 == 0: \n",
    "\n",
    "      # save embeddings for future visualization \n",
    "      path = os.path.join(\"model_embeddings\", model.name)\n",
    "      if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "      torch.save(model.embedding.weight, os.path.join(\"model_embeddings\", model.name, f\"{model.name}_{loss_fn}_{neg_samp}_{epoch}.pt\"))\n",
    "\n",
    "  pickle.dump(stats, open(f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pkl\", \"wb\"))\n",
    "  return stats\n",
    "\n",
    "\n",
    "def test(model, data, loss_fn, neg_samp, epoch = 0, neg_edge_index = None, neg_edge_label = None):\n",
    "  \n",
    "  model.eval()\n",
    "  with torch.no_grad(): # want to save RAM \n",
    "\n",
    "    # conduct negative sampling \n",
    "    if neg_samp == \"random\":\n",
    "      neg_edge_index, neg_edge_label = sample_negative_edges(data, n_user, n_item, args[\"device\"])\n",
    "    elif neg_samp == \"hard\":\n",
    "      if epoch % 5 == 0 or neg_edge_index is None: \n",
    "        neg_edge_index, neg_edge_label = sample_hard_negative_edges(\n",
    "            data, model, n_user, n_item, args[\"device\"], batch_size = 500,\n",
    "            frac_sample = 1 - (0.5 * epoch / args[\"epochs\"])\n",
    "        )\n",
    "    # obtain model embedding\n",
    "    embed = model.get_embedding(data.edge_index)\n",
    "    # calculate pos, neg scores using embedding \n",
    "    pos_scores = model.predict_link_embedding(embed, data.edge_label_index)\n",
    "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
    "    # concatenate pos, neg scores together and evaluate loss \n",
    "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
    "    labels = torch.cat((data.edge_label, neg_edge_label), dim = 0)\n",
    "    # calculate loss \n",
    "    if loss_fn == \"BCE\": \n",
    "      loss = model.link_pred_loss(scores, labels)\n",
    "    elif loss_fn == \"BPR\":\n",
    "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
    "\n",
    "    roc = metrics(labels, scores)\n",
    "    \n",
    "  return loss, roc, neg_edge_index, neg_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the dataset splits \n",
    "datasets = {\n",
    "    'train':train_split, \n",
    "    'val':val_split, \n",
    "    'test': test_split \n",
    "}\n",
    "\n",
    "# initialize our arguments \n",
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_layers' :  3,\n",
    "    'emb_size' : 64,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 301\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and and optimizer \n",
    "num_nodes = n_user + n_item\n",
    "model = GCN(\n",
    "    num_nodes = num_nodes, num_layers = args['num_layers'], \n",
    "    embedding_dim = args[\"emb_size\"], conv_layer = \"SAGE\"\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data, model to GPU if available\n",
    "user_idx = torch.Tensor(user_idx).type(torch.int64).to(args[\"device\"])\n",
    "item_idx =torch.Tensor(item_idx).type(torch.int64).to(args[\"device\"])\n",
    "datasets['train'].to(args['device'])\n",
    "datasets['val'].to(args['device'])\n",
    "datasets['test'].to(args['device'])\n",
    "model.to(args[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save model_stats\n",
    "MODEL_STATS_DIR = \"model_stats\"\n",
    "if not os.path.exists(MODEL_STATS_DIR):\n",
    "  os.makedirs(MODEL_STATS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(datasets, model, optimizer, \"BPR\", args, neg_samp = \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, datasets['test'], \"BPR\", neg_samp = \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(conv_layer, args, alpha = False):\n",
    "  num_nodes = n_nodes\n",
    "  model = GCN(\n",
    "      num_nodes = num_nodes, num_layers = args['num_layers'], \n",
    "      embedding_dim = args[\"emb_size\"], conv_layer = conv_layer, \n",
    "      alpha_learnable = alpha\n",
    "  )\n",
    "  model.to(args[\"device\"])\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "  return model, optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For example:\n",
    "\n",
    "# using BPR loss\n",
    "loss_fn = \"BPR\"\n",
    "\n",
    "# using hard sampling\n",
    "neg_samp = \"hard\"\n",
    "\n",
    "# for LGConv: \n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 4\n",
    "model, optimizer = init_model(\"LGC\", args)\n",
    "lgc_stats_hard = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GATConv: \n",
    "loss_fn = \"BPR\"\n",
    "neg_samp = \"hard\"\n",
    "args['epochs'] = 150\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"GAT\", args)\n",
    "gat_stats_hard = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SAGEConv: \n",
    "neg_samp = \"hard\"\n",
    "args['epochs'] = 150\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"SAGE\", args)\n",
    "sage_stats_hard = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using random sampling\n",
    "neg_samp = \"random\"\n",
    "\n",
    "# for LGConv: \n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 4\n",
    "model, optimizer = init_model(\"LGC\", args)\n",
    "lgc_stats = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GATConv: \n",
    "neg_samp = \"random\"\n",
    "args['epochs'] = 150\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"GAT\", args)\n",
    "gat_stats = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SAGEConv: \n",
    "neg_samp = \"random\"\n",
    "args['epochs'] = 150\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"SAGE\", args)\n",
    "sage_stats = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detach_loss(stats): \n",
    "  return [loss.detach().cpu().numpy().item() for loss in stats]\n",
    "\n",
    "def plot_train_val_loss(stats_dict):\n",
    "  fig, ax = plt.subplots(1,1, figsize = (6, 4))\n",
    "  train_loss = detach_loss(stats_dict[\"train\"][\"loss\"])\n",
    "  val_loss = detach_loss(stats_dict[\"val\"][\"loss\"])\n",
    "  idx = np.arange(0, len(train_loss), 1)\n",
    "  ax.plot(idx, train_loss, label = \"train\")\n",
    "  ax.plot(idx, val_loss, label = \"val\")\n",
    "  ax.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgc_stats = pickle.load(open(f\"model_stats/LGCN_LGC_4_e64_nodes17738__BPR_random.pkl\", \"rb\"))\n",
    "lgc_stats_hard = pickle.load(open(f\"model_stats/LGCN_LGC_4_e64_nodes17738__BPR_hard.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you had to stop for whatever reason, you can always reload the stats here! (just uncomment and change to correct paths)\n",
    "gat_stats = pickle.load(open(f\"model_stats\\LGCN_GAT_3_e64_nodes17738__BPR_random.pkl\", \"rb\"))\n",
    "sage_stats = pickle.load(open(f\"model_stats\\LGCN_SAGE_3_e64_nodes17738__BPR_random.pkl\", \"rb\"))\n",
    "\n",
    "gat_stats_hard = pickle.load(open(f\"model_stats\\LGCN_GAT_3_e64_nodes17738__BPR_hard.pkl\", \"rb\"))\n",
    "sage_stats_hard = pickle.load(open(f\"model_stats\\LGCN_SAGE_3_e64_nodes17738__BPR_hard.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(lgc_stats)\n",
    "plot_train_val_loss(lgc_stats_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(sage_stats)\n",
    "plot_train_val_loss(sage_stats_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(gat_stats)\n",
    "plot_train_val_loss(gat_stats_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
    "key = \"loss\"\n",
    "lgc_loss = pd.Series(detach_loss(lgc_stats[\"val\"][key])).rolling(3).mean()\n",
    "gat_loss = pd.Series(detach_loss(gat_stats[\"val\"][key])).rolling(3).mean()\n",
    "sage_loss = pd.Series(detach_loss(sage_stats[\"val\"][key])).rolling(3).mean()\n",
    "idx = np.arange(0, len(lgc_loss), 1)\n",
    "idx2 = np.arange(0, len(gat_loss), 1)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "ax.plot(idx, lgc_loss, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
    "ax.plot(idx2, gat_loss, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
    "ax.plot(idx2, sage_loss, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
    "ax.legend(loc = 'lower right')\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"BPR Loss\")\n",
    "ax.set_title(\"Model BPR Loss, by convolution type and negative sampling\")\n",
    "ax.set_ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
    "key = \"loss\"\n",
    "lgc_loss = pd.Series(detach_loss(lgc_stats[\"val\"][key])).rolling(3).mean()\n",
    "gat_loss = pd.Series(detach_loss(gat_stats[\"val\"][key])).rolling(3).mean()\n",
    "sage_loss = pd.Series(detach_loss(sage_stats[\"val\"][key])).rolling(3).mean()\n",
    "lgc_hard_loss = pd.Series(detach_loss(lgc_stats_hard[\"val\"][key])).rolling(3).mean()\n",
    "gat_hard_loss = pd.Series(detach_loss(gat_stats_hard[\"val\"][key])).rolling(3).mean()\n",
    "sage_hard_loss = pd.Series(detach_loss(sage_stats_hard[\"val\"][key])).rolling(3).mean()\n",
    "idx = np.arange(0, len(lgc_loss), 1)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "ax.plot(idx, lgc_loss, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
    "ax.plot(idx, lgc_hard_loss, color = colors[0], label = \"LGC - hard\")\n",
    "ax.plot(idx2, gat_loss, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
    "ax.plot(idx2, gat_hard_loss, color = colors[1], label = \"GAT - hard\")\n",
    "ax.plot(idx2, sage_loss, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
    "ax.plot(idx2, sage_hard_loss, color = colors[2], label = \"SAGE - hard\")\n",
    "ax.legend(loc = 'lower left')\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"BPR Loss\")\n",
    "ax.set_title(\"Model BPR Loss, by convolution type and negative sampling\")\n",
    "ax.set_ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
    "key = \"recall\"\n",
    "lgc_recall = lgc_stats[\"val\"][key]\n",
    "gat_recall = gat_stats[\"val\"][key]\n",
    "sage_recall = sage_stats[\"val\"][key]\n",
    "lgc_hard_recall = lgc_stats_hard[\"val\"][key]\n",
    "gat_hard_recall = gat_stats_hard[\"val\"][key]\n",
    "sage_hard_recall = sage_stats_hard[\"val\"][key]\n",
    "# increment by 10\n",
    "idx = np.arange(0, 10 * len(lgc_recall), 10)\n",
    "idx2 = np.arange(0, 10 * len(gat_recall), 10)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "ax.plot(idx, lgc_recall, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
    "ax.plot(idx, lgc_hard_recall, color = colors[0], label = \"LGC - hard\")\n",
    "ax.plot(idx2, gat_recall, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
    "ax.plot(idx2, gat_hard_recall, color = colors[1], label = \"GAT - hard\")\n",
    "ax.plot(idx2, sage_recall, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
    "ax.plot(idx2, sage_hard_recall, color = colors[2], label = \"SAGE - hard\")\n",
    "ax.legend(loc = 'lower right')\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Recall@300\")\n",
    "ax.set_title(\"Model Recall@300, by convolution type and negative sampling\")\n",
    "ax.set_ylim(0, 0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
